{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Create working folders for tutorial"
      ],
      "metadata": {
        "id": "9ZaliysAEbOG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p /content/dep_collocation_tutorial\n",
        "!mkdir -p /content/dep_collocation_tutorial/OANC\n",
        "!mkdir -p /content/dep_collocation_tutorial/NICER\n",
        "!mkdir -p /content/dep_collocation_tutorial/NICER/nicer_plain\n",
        "!mkdir -p /content//dep_collocation_tutorial/collocation\n",
        "\n",
        "# /content\n",
        "# 　　└── dep_collocation_tutorial\n",
        "#     　　　　　├── OANC <-- reference corpus\n",
        "#     　　　　　├── NICER\n",
        "#     　　　　　│  　　 └── nicer_plain <-- learner corpus\n",
        "#     　　　　　└── collocation <-- extracted collocation\n"
      ],
      "metadata": {
        "id": "Xz3-TMndDYx0"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Create a Python library (mylib.py)"
      ],
      "metadata": {
        "id": "-v0ptsrmEn9P"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4c-kB5VvC8bJ"
      },
      "outputs": [],
      "source": [
        "# Create a Python library\n",
        "\n",
        "%%writefile /content/dep_collocation_tutorial/mylib.py\n",
        "\n",
        "EXCLUDE_OBJ_WORD = {\"my\", \"your\", \"her\", \"its\", \"our\", \"their\", \"a\", \"an\", \"the\"}\n",
        "SUBJECT_DEPS = {\"nsubj\", \"nsubjpass\", \"expl\"}\n",
        "EXCLUDE_ADVERBS = {\n",
        "    \"however\", \"therefore\", \"thus\", \"furthermore\", \"moreover\", \"nevertheless\",\n",
        "    \"nonetheless\", \"instead\", \"consequently\", \"accordingly\", \"meanwhile\",\n",
        "    \"besides\", \"otherwise\", \"hence\", \"so\", \"how\", \"why\", \"when\"\n",
        "}\n",
        "\n",
        "def has_subject_between(left_tok, verb_tok, sent):\n",
        "    i1, i2 = left_tok.i, verb_tok.i\n",
        "    for t in sent:\n",
        "        if i1 < t.i < i2 and t.dep_ in SUBJECT_DEPS and t.head == verb_tok:\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "def is_followed_by_comma(tok):\n",
        "    if tok.i + 1 >= len(tok.doc):\n",
        "        return False\n",
        "    next_token = tok.doc[tok.i + 1]\n",
        "    return next_token.is_punct and next_token.text == \",\"\n",
        "\n",
        "def is_how_plus_adv(tok):\n",
        "    if tok.pos_ != \"ADV\":\n",
        "        return False\n",
        "    if tok.i - 1 >= 0:\n",
        "        prev_token = tok.doc[tok.i - 1]\n",
        "        return prev_token.text.lower() == \"how\"\n",
        "    return False\n",
        "\n",
        "def is_hyphenated_token(tok):\n",
        "    \"\"\"\n",
        "    トークン tok がハイフン結合語なら True を返す。\n",
        "    - \"part\" \"-\" \"time\" → \"part\" と \"time\" が True\n",
        "    \"\"\"\n",
        "    HYPHENS = {\"-\", \"‐\", \"‒\", \"–\", \"−\"}  # hyphen, en dash, minus など\n",
        "    text = tok.text\n",
        "    # トークン自身がハイフン記号なら True\n",
        "    if text in HYPHENS:\n",
        "        return True\n",
        "    # 直前または直後がハイフン記号トークンの場合\n",
        "    doc = tok.doc\n",
        "    i = tok.i\n",
        "    if i - 1 >= 0 and doc[i - 1].text in HYPHENS:\n",
        "        return True\n",
        "    if i + 1 < len(doc) and doc[i + 1].text in HYPHENS:\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "def is_excluded_ing(token) -> bool:\n",
        "    \"\"\"\n",
        "    token が -ing 語で、例外リストに含まれず、\n",
        "    直前のトークンが VERB の場合に True。\n",
        "    \"\"\"\n",
        "    EXCEPT_ING = {\"thing\", \"something\", \"anything\", \"nothing\"}\n",
        "    word = token.lower_\n",
        "\n",
        "    if word.endswith(\"ing\") and word not in EXCEPT_ING:\n",
        "        doc = token.doc\n",
        "        if token.i > 0 and doc[token.i - 1].pos_ == \"VERB\":\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "# 判定ヘルパー\n",
        "def is_adj_noun(token):\n",
        "    return (\n",
        "        token.dep_ in (\"amod\", \"compound\")\n",
        "        and token.head.pos_ == \"NOUN\"\n",
        "        and token.i < token.head.i\n",
        "        and not is_hyphenated_token(token)\n",
        "        and not is_hyphenated_token(token.head)\n",
        "    )\n",
        "\n",
        "def is_verb_obj(token):\n",
        "    return (\n",
        "        token.dep_ in {\"dobj\"}\n",
        "        and token.text.lower() not in EXCLUDE_OBJ_WORD\n",
        "        and not is_excluded_ing(token)\n",
        "        and token.head.pos_ == \"VERB\"\n",
        "        and token.head.lemma_ != \"be\"\n",
        "        and token.head.i < token.i\n",
        "        and not is_hyphenated_token(token)\n",
        "        and not is_hyphenated_token(token.head)\n",
        "    )\n",
        "\n",
        "def is_verb_adv(token):\n",
        "    return (\n",
        "        token.dep_ in (\"advmod\", \"npadvmod\", \"prt\")\n",
        "        and token.head.pos_ == \"VERB\"\n",
        "        and token.head.lemma_ != \"be\"\n",
        "        and token.head.i < token.i\n",
        "        and not is_hyphenated_token(token)\n",
        "        and not is_hyphenated_token(token.head)\n",
        "    )\n",
        "\n",
        "def is_adv_verb(token, sent):\n",
        "    return (\n",
        "        token.dep_ in (\"advmod\", \"npadvmod\")\n",
        "        and token.head.pos_ == \"VERB\"\n",
        "        and token.head.lemma_ != \"be\"\n",
        "        and token.i < token.head.i\n",
        "        and token.text.lower() not in EXCLUDE_ADVERBS\n",
        "        and not is_followed_by_comma(token)\n",
        "        and not has_subject_between(token, token.head, sent)\n",
        "        and not is_how_plus_adv(token)\n",
        "        and not is_hyphenated_token(token)\n",
        "        and not is_hyphenated_token(token.head)\n",
        "    )\n",
        "\n",
        "def is_adv_adj(token):\n",
        "    return (\n",
        "        token.dep_ in (\"advmod\", \"npadvmod\")\n",
        "        and token.head.pos_ == \"ADJ\"\n",
        "        and token.i < token.head.i\n",
        "        and not is_hyphenated_token(token)\n",
        "        and not is_hyphenated_token(token.head)\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3. Dowonload learner corpus (NICER)\n",
        "✅**Replace 'xxx'** in the code below with the appropriate username and password<br>\n",
        "Refer to https://sugiura-ken.org/sgr/nicer/nicer-1-3-2/"
      ],
      "metadata": {
        "id": "rJbT6SlgE3zv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget --user=\"xxx\" --password=\"xxx\" \"http://venus.hum.nagoya-u.ac.jp/nice-download/NICER1_3_2.zip\" -O /content/dep_collocation_tutorial/NICER/NICER1_3_2.zip"
      ],
      "metadata": {
        "id": "AtXbSOwIFHe5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Unzip the file\n",
        "# A folder named \"2020-11-24NICER1_3_2\" will be created.\n",
        "\n",
        "!unzip -d /content/dep_collocation_tutorial/NICER /content/dep_collocation_tutorial/NICER/NICER1_3_2.zip"
      ],
      "metadata": {
        "id": "EC_5deH3HH5A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract only learner essays from the NICER corpus\n",
        "# Convert CHAT format to plain text\n",
        "# Output the texts to the 'nicer_plain' folder\n",
        "\n",
        "import os\n",
        "import re\n",
        "from pathlib import Path\n",
        "from glob import glob\n",
        "\n",
        "# === Input / Output Folders ===\n",
        "INPUT_DIR = \"/content/dep_collocation_tutorial/NICER/2020-11-24NICER1_3_2/NICER_NNS\"\n",
        "OUTPUT_DIR = \"/content/dep_collocation_tutorial/NICER/nicer_plain\"\n",
        "\n",
        "# Create the output folder\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# Extraction pattern: lines starting with *JPN001:, etc.\n",
        "pattern = re.compile(r'^\\*JPN[0-9]+:\\s*')\n",
        "\n",
        "# Sort the list of input files\n",
        "txt_files = sorted(glob(str(Path(INPUT_DIR) / \"*.txt\")))\n",
        "\n",
        "print(f\"{len(txt_files)} text files found in {INPUT_DIR}\")\n",
        "\n",
        "processed_count = 0\n",
        "\n",
        "for i, fp in enumerate(txt_files, start=1):\n",
        "    p = Path(fp)\n",
        "    selected = []\n",
        "    with open(fp, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
        "        for line in f:\n",
        "            if pattern.match(line):\n",
        "                cleaned = pattern.sub(\"\", line)\n",
        "                if not cleaned.endswith(\"\\n\"):\n",
        "                    cleaned += \"\\n\"\n",
        "                selected.append(cleaned)\n",
        "\n",
        "    # ★ Skip the first line (already cleaned)\n",
        "    selected = selected[1:]\n",
        "\n",
        "    out_path = Path(OUTPUT_DIR) / f\"{p.stem}_plain.txt\"\n",
        "\n",
        "    with open(out_path, \"w\", encoding=\"utf-8\", errors=\"ignore\") as wf:\n",
        "        wf.writelines(selected)\n",
        "\n",
        "    processed_count += 1\n",
        "    print(f\"[{i}/{len(txt_files)}] Saved: {out_path.name} (lines: {len(selected)})\")\n",
        "\n",
        "print(\"✅ Done.\")\n",
        "print(f\"Total processed files: {processed_count}\")\n"
      ],
      "metadata": {
        "id": "3xXTChRWJHUE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4. Extract dependency pairs from NICER learner corpus"
      ],
      "metadata": {
        "id": "E36uo1SPJzoL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# pair_extractor.py\n",
        "import os, sys, time, threading, re, shutil\n",
        "import pandas as pd\n",
        "import spacy\n",
        "\n",
        "# ===== Environment Check =====\n",
        "try:\n",
        "    import google.colab  # type: ignore\n",
        "    IN_COLAB = True\n",
        "except ImportError:\n",
        "    IN_COLAB = False\n",
        "\n",
        "# ===== Path Settings =====\n",
        "if IN_COLAB:\n",
        "    # colab\n",
        "    #from google.colab import drive\n",
        "    #drive.mount(\"/content/drive\", force_remount=False)\n",
        "\n",
        "    #WORKING_DIR_PATH = \"/content/drive/MyDrive/dep_collocation_tutorial\"\n",
        "    WORKING_DIR_PATH = \"/content/dep_collocation_tutorial\"\n",
        "    CORPUS_DIR = \"NICER/nicer_plain\"\n",
        "    INPUT_ROOT = os.path.join(WORKING_DIR_PATH, CORPUS_DIR) # /content/dep_collocation_tutorial/NICER/nicer_plain\n",
        "    OUTPUT_DIR = os.path.join(WORKING_DIR_PATH, \"collocation\") # /content/dep_collocation_tutorial/collocation\n",
        "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "    sys.path.append(WORKING_DIR_PATH) # Specify the directory to load the library 'mylib'\n",
        "else:\n",
        "    # Local environment\n",
        "    sys.path.append(os.path.abspath(os.path.dirname(__file__)))# Look for the library in the directory where the current script is located\n",
        "\n",
        "import mylib  # is_adj_noun / is_verb_obj / is_verb_adv / is_adv_verb / is_adv_adj\n",
        "\n",
        "# ===== Common Utilities =====\n",
        "def get_all_txt_files(root_folder: str):\n",
        "    txt_files = []\n",
        "    for dirpath, _, filenames in os.walk(root_folder):\n",
        "        for f in filenames:\n",
        "            if f.lower().endswith(\".txt\"):\n",
        "                txt_files.append(os.path.join(dirpath, f))\n",
        "    return sorted(txt_files, key=lambda x: os.path.basename(x))\n",
        "\n",
        "def add_result(results, file_path, pair_type, sentence_text, i1, i2, token1, token2, span):\n",
        "    ZERO_WIDTH = \"\".join([\"\\u200b\", \"\\u200c\", \"\\u200d\", \"\\ufeff\"])  # ZWSP/ZWNJ/ZWJ/BOM\n",
        "    lemma1 = token1.lemma_.lower()\n",
        "    lemma2 = token2.lemma_.lower()\n",
        "    lemma1 = re.sub(f\"[{ZERO_WIDTH}]\", \"\", lemma1)\n",
        "    lemma2 = re.sub(f\"[{ZERO_WIDTH}]\", \"\", lemma2)\n",
        "    results.append({\n",
        "        \"file\": os.path.basename(file_path),\n",
        "        \"pair_type\": pair_type,\n",
        "        \"sentence\": sentence_text,\n",
        "        \"i1\": i1, \"i2\": i2,\n",
        "        \"lemma1\": lemma1, \"lemma2\": lemma2,\n",
        "        \"word1\": token1.text, \"word2\": token2.text,\n",
        "        \"pos1\": token1.pos_, \"pos2\": token2.pos_,\n",
        "        \"tag1\": token1.tag_, \"tag2\": token2.tag_,\n",
        "        \"dep1\": token1.dep_, \"dep2\": token2.dep_,\n",
        "        \"pair\": f\"{token1.text} - {token2.text}\",\n",
        "        \"lemma_pair\": f\"{lemma1} - {lemma2}\",\n",
        "        \"span\": span\n",
        "    })\n",
        "\n",
        "def build_nlp():\n",
        "    nlp = spacy.load(\"en_core_web_sm\", disable=[\"ner\", \"textcat\"])\n",
        "    if \"sentencizer\" not in nlp.pipe_names:\n",
        "        nlp.add_pipe(\"sentencizer\", first=True)\n",
        "    return nlp\n",
        "\n",
        "def process_files(INPUT_ROOT: str, OUTPUT_DIR: str, log_func=print):\n",
        "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "    txt_files = get_all_txt_files(INPUT_ROOT)\n",
        "    if not txt_files:\n",
        "        log_func(f\"ERROR: No .txt files in {INPUT_ROOT}\")\n",
        "        return\n",
        "\n",
        "    nlp_parser = build_nlp()\n",
        "    all_results = []\n",
        "    t0 = time.time()\n",
        "\n",
        "    for idx, file_path in enumerate(txt_files, 1):\n",
        "        try:\n",
        "            rel = os.path.relpath(file_path, INPUT_ROOT)\n",
        "        except ValueError:\n",
        "            rel = os.path.basename(file_path)\n",
        "\n",
        "        try:\n",
        "            with open(file_path, \"r\", encoding=\"utf-8\", errors=\"replace\") as f:\n",
        "                lines = f.read().splitlines()\n",
        "        except Exception as e:\n",
        "            log_func(f\"[WARN] Skip (read error): {rel} -> {e}\")\n",
        "            continue\n",
        "\n",
        "        msg = f\"files {idx}/{len(txt_files)} : {rel}\"\n",
        "        if log_func is print:\n",
        "            print(\"\\r\" + msg, end=\"\", flush=True)\n",
        "        else:\n",
        "            log_func(msg)\n",
        "\n",
        "        file_results = []\n",
        "        for line_text in lines:\n",
        "            if not line_text.strip():\n",
        "                continue\n",
        "            doc_line = nlp_parser(line_text)\n",
        "            for sent in doc_line.sents:\n",
        "                s_start = sent.start\n",
        "                for token in sent:\n",
        "                    if mylib.is_adj_noun(token):\n",
        "                        i1, i2 = token.i - s_start, token.head.i - s_start\n",
        "                        if i1 > i2: i1, i2 = i2, i1\n",
        "                        span = sent[i1:i2+1].text\n",
        "                        add_result(file_results, rel, \"ADJ_NOUN\", sent.text, i1, i2, token, token.head, span)\n",
        "                    if mylib.is_verb_obj(token):\n",
        "                        i1, i2 = token.head.i - s_start, token.i - s_start\n",
        "                        span = sent[i1:i2+1].text\n",
        "                        add_result(file_results, rel, \"VERB_OBJ\", sent.text, i1, i2, token.head, token, span)\n",
        "                    if mylib.is_verb_adv(token):\n",
        "                        i1, i2 = token.head.i - s_start, token.i - s_start\n",
        "                        span = sent[i1:i2+1].text\n",
        "                        add_result(file_results, rel, \"VERB_ADV\", sent.text, i1, i2, token.head, token, span)\n",
        "                    if mylib.is_adv_verb(token, sent):\n",
        "                        i1, i2 = token.i - s_start, token.head.i - s_start\n",
        "                        span = sent[i1:i2+1].text\n",
        "                        add_result(file_results, rel, \"ADV_VERB\", sent.text, i1, i2, token, token.head, span)\n",
        "                    if mylib.is_adv_adj(token):\n",
        "                        i1, i2 = token.i - s_start, token.head.i - s_start\n",
        "                        span = sent[i1:i2+1].text\n",
        "                        add_result(file_results, rel, \"ADV_ADJ\", sent.text, i1, i2, token, token.head, span)\n",
        "\n",
        "        all_results.extend(file_results)\n",
        "\n",
        "    df_all = pd.DataFrame(all_results)\n",
        "    if not df_all.empty:\n",
        "        out_all = os.path.join(OUTPUT_DIR, \"pairs_all.csv\")\n",
        "        df_all.to_csv(out_all, index=False, encoding=\"utf-8-sig\")\n",
        "        for pt in [\"ADJ_NOUN\", \"VERB_OBJ\", \"VERB_ADV\", \"ADV_ADJ\", \"ADV_VERB\"]:\n",
        "            sub = df_all[df_all[\"pair_type\"] == pt]\n",
        "            if not sub.empty:\n",
        "                freq = sub[\"lemma_pair\"].value_counts().reset_index()\n",
        "                freq.columns = [\"lemma_pair\", \"frequency\"]\n",
        "                freq.to_csv(os.path.join(OUTPUT_DIR, f\"frequencies_{pt}.csv\"), index=False, encoding=\"utf-8-sig\")\n",
        "        log_func(f\"\\n✓ Results saved to: {OUTPUT_DIR}\")\n",
        "    else:\n",
        "        log_func(\"\\nNo dependency pairs were extracted.\")\n",
        "    log_func(f\"✓ Total time: {time.time()-t0:.2f} sec\")\n",
        "\n",
        "# ===== Entry Point =====\n",
        "if IN_COLAB:\n",
        "    print(\"[Colab] INPUT_DIR:\", INPUT_ROOT)\n",
        "\n",
        "    if not os.path.isdir(INPUT_ROOT):\n",
        "        raise FileNotFoundError(f\"Not found: {INPUT_ROOT}\")\n",
        "\n",
        "    if \"/content/drive\" in INPUT_ROOT:\n",
        "        # ===== Cache Settings (Google Drive path) =====\n",
        "        shutil.copytree(INPUT_ROOT, \"/content/text_cache\", dirs_exist_ok=True)\n",
        "        INPUT_CACHE = \"/content/text_cache\"\n",
        "        print(\"[Colab] Copied to :\", INPUT_CACHE)\n",
        "        print(\"[Colab] Using cache ...\")\n",
        "    else:\n",
        "        # ===== Direct Processing (virtual / local path) =====\n",
        "        INPUT_CACHE = INPUT_ROOT\n",
        "        print(\"[Colab] Using virtual drive path directly ...\")\n",
        "\n",
        "    print(\"[Colab] OUTPUT_DIR:\", OUTPUT_DIR)\n",
        "    process_files(INPUT_CACHE, OUTPUT_DIR, log_func=print)\n",
        "\n",
        "else:\n",
        "    # --- Local GUI: Select input/output via GUI and start ---\n",
        "    import tkinter as tk\n",
        "    from tkinter import filedialog, messagebox, ttk\n",
        "\n",
        "    class App(tk.Tk):\n",
        "        def __init__(self):\n",
        "            super().__init__()\n",
        "            self.title(\"PairExtractor\")\n",
        "            self.geometry(\"720x520\")\n",
        "            self.input_dir = tk.StringVar()\n",
        "            self.output_dir = tk.StringVar()\n",
        "            # Specify the PNG file\n",
        "            #icon = tk.PhotoImage(file=\"icon.png\")\n",
        "            #self.iconphoto(True, icon)\n",
        "\n",
        "            pad = {\"padx\": 10, \"pady\": 8}\n",
        "\n",
        "            frm_in = ttk.LabelFrame(self, text=\"Input\")\n",
        "            frm_in.pack(fill=\"x\", **pad)\n",
        "            ttk.Button(frm_in, text=\"Input folder\", command=self.select_input).pack(anchor=\"w\", padx=10, pady=(10,4))\n",
        "            ttk.Entry(frm_in, textvariable=self.input_dir, state=\"readonly\").pack(fill=\"x\", padx=10, pady=(0,10))\n",
        "\n",
        "            frm_out = ttk.LabelFrame(self, text=\"Output\")\n",
        "            frm_out.pack(fill=\"x\", **pad)\n",
        "            ttk.Button(frm_out, text=\"Output folder\", command=self.select_output).pack(anchor=\"w\", padx=10, pady=(10,4))\n",
        "            ttk.Entry(frm_out, textvariable=self.output_dir, state=\"readonly\").pack(fill=\"x\", padx=10, pady=(0,10))\n",
        "\n",
        "            frm_start = ttk.Frame(self); frm_start.pack(fill=\"x\", **pad)\n",
        "            self.btn_start = ttk.Button(frm_start, text=\"Extract start\", command=self.on_start, state=\"disabled\")\n",
        "            self.btn_start.pack(side=\"left\")\n",
        "            self.lbl_status = ttk.Label(frm_start, text=\"Select input/output folders.\")\n",
        "            self.lbl_status.pack(side=\"left\", padx=10)\n",
        "\n",
        "            frm_log = ttk.LabelFrame(self, text=\"Log\")\n",
        "            frm_log.pack(fill=\"both\", expand=True, padx=10, pady=10)\n",
        "            cnt = ttk.Frame(frm_log)\n",
        "            cnt.pack(fill=\"both\", expand=True)\n",
        "            self.txt_log = tk.Text(cnt, height=14, wrap=\"none\")\n",
        "            self.txt_log.pack(side=\"left\", fill=\"both\", expand=True)\n",
        "            self.scroll_y = ttk.Scrollbar(cnt, orient=\"vertical\", command=self.txt_log.yview)\n",
        "            self.scroll_y.pack(side=\"right\", fill=\"y\")\n",
        "            self.txt_log.configure(yscrollcommand=self.scroll_y.set)\n",
        "\n",
        "        def select_input(self):\n",
        "            p = filedialog.askdirectory(title=\"Select input folder (e.g., JPN_text)\")\n",
        "            if p:\n",
        "                self.input_dir.set(p); self.update_state()\n",
        "\n",
        "        def select_output(self):\n",
        "            p = filedialog.askdirectory(title=\"Select output folder (empty or existing)\")\n",
        "            if p:\n",
        "                self.output_dir.set(p); self.update_state()\n",
        "\n",
        "        def update_state(self):\n",
        "            self.btn_start.config(state=(tk.NORMAL if self.input_dir.get() and self.output_dir.get() else tk.DISABLED))\n",
        "\n",
        "        def log(self, msg):\n",
        "            self.txt_log.insert(\"end\", msg + \"\\n\"); self.txt_log.see(\"end\"); self.update_idletasks()\n",
        "\n",
        "        def on_start(self):\n",
        "            in_p, out_p = self.input_dir.get().strip(), self.output_dir.get().strip()\n",
        "            if not in_p or not out_p:\n",
        "                messagebox.showwarning(\"Warning\", \"Both input and output folders must be selected.\"); return\n",
        "            self.btn_start.config(state=\"disabled\"); self.lbl_status.config(text=\"Running...\")\n",
        "            threading.Thread(target=self._run, args=(in_p, out_p), daemon=True).start()\n",
        "\n",
        "        def _run(self, in_p, out_p):\n",
        "            try:\n",
        "                process_files(in_p, out_p, log_func=self.log)\n",
        "                self.lbl_status.config(text=\"Done.\")\n",
        "            except Exception as e:\n",
        "                self.log(f\"[ERROR] {e}\"); messagebox.showerror(\"Error\", str(e)); self.lbl_status.config(text=\"Error.\")\n",
        "            finally:\n",
        "                self.btn_start.config(state=\"normal\")\n",
        "\n",
        "    if __name__ == \"__main__\":\n",
        "        App().mainloop()"
      ],
      "metadata": {
        "id": "tILnmL4zKFyM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#5. Download OANC\n",
        "See below for details on the OANC corpus<br>\n",
        "https://anc.org/"
      ],
      "metadata": {
        "id": "0UCHxYulOSlA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download OANC (use --no-check-certificate to avoid certificate errors)\n",
        "!wget --no-check-certificate -O /content/dep_collocation_tutorial/OANC/OANC.zip \\\n",
        "  https://www.anc.org/OANC/OANC-1.0.1-UTF8.zip\n",
        "\n",
        "# Unzip to /content/dep_collocation_tutorial/OANC\n",
        "!unzip -o /content/dep_collocation_tutorial/OANC/OANC.zip -d /content/dep_collocation_tutorial/OANC"
      ],
      "metadata": {
        "id": "yWJ_ZxKOOt6z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the tutorial, copy only:\n",
        "- written_1/journal/verbatim\n",
        "- written_2/non-fiction/OUP"
      ],
      "metadata": {
        "id": "WVFE25qpPu1q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "import shutil\n",
        "\n",
        "# Original root and list of source directories to copy\n",
        "base_dir = Path(\"/content/dep_collocation_tutorial/OANC/OANC\")\n",
        "input_dirs = [\n",
        "    base_dir / \"data\" / \"written_1\" / \"journal\" / \"verbatim\",\n",
        "    base_dir / \"data\" / \"written_2\" / \"non-fiction\" / \"OUP\",\n",
        "]\n",
        "\n",
        "# Copy destination\n",
        "dest_root = Path(\"/content/dep_collocation_tutorial/OANC/text_selected\")\n",
        "dest_root.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "copied, skipped = 0, 0\n",
        "\n",
        "for src_root in input_dirs:\n",
        "    if not src_root.is_dir():\n",
        "        print(f\"[SKIP] Not found: {src_root}\")\n",
        "        continue\n",
        "    for src in src_root.rglob(\"*.txt\"):\n",
        "        # Preserve relative paths from base_dir during copy\n",
        "        rel = src.relative_to(base_dir)\n",
        "        dest = dest_root / rel\n",
        "        dest.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        shutil.copy2(src, dest)\n",
        "        copied += 1\n",
        "\n",
        "\n",
        "print(f\"Copied {copied} files, skipped {skipped}, into {dest_root} (structure preserved)\")\n",
        "\n"
      ],
      "metadata": {
        "id": "vXJ613kvd9Rk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fix sentence-internal line breaks in OANC texts"
      ],
      "metadata": {
        "id": "YgMDzgRUeGW6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "from glob import glob\n",
        "\n",
        "SRC_ROOT = \"/content/dep_collocation_tutorial/OANC/text_selected\"\n",
        "DST_ROOT = \"/content/dep_collocation_tutorial/OANC/text_selected_norm\"\n",
        "\n",
        "import re\n",
        "\n",
        "KAIGYO = \"###KAIGYO###\"\n",
        "DANRAKU = \"###DANRAKU###\"\n",
        "\n",
        "def normalize_with_tokens(text: str) -> str:\n",
        "    text = text.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\n",
        "    text = text.replace(\"\\n\", KAIGYO)\n",
        "    text = re.sub(r'(?:' + re.escape(KAIGYO) + r'){2,}', DANRAKU, text)\n",
        "    text = text.replace(KAIGYO, \" \")\n",
        "    text = text.replace(DANRAKU, \"\\n\")\n",
        "    text = re.sub(r'[ \\t]+', ' ', text)\n",
        "    text = re.sub(r' *\\n *', '\\n', text)\n",
        "    return text.strip()\n",
        "\n",
        "\n",
        "def read_text(path: str) -> str:\n",
        "    for enc in (\"utf-8\", \"utf-8-sig\", \"latin-1\"):\n",
        "        try:\n",
        "            with open(path, \"r\", encoding=enc, errors=\"strict\") as f:\n",
        "                return f.read()\n",
        "        except Exception:\n",
        "            continue\n",
        "    with open(path, \"r\", encoding=\"utf-8\", errors=\"replace\") as f:\n",
        "        return f.read()\n",
        "\n",
        "def write_text(path: str, text: str) -> None:\n",
        "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
        "    with open(path, \"w\", encoding=\"utf-8\", newline=\"\\n\") as f:\n",
        "        f.write(text)\n",
        "\n",
        "def main():\n",
        "    files = glob(os.path.join(SRC_ROOT, \"**\", \"*.txt\"), recursive=True)\n",
        "    if not files:\n",
        "        print(f\"No .txt files found under: {SRC_ROOT}\")\n",
        "        return\n",
        "\n",
        "    processed = 0\n",
        "    skipped = 0\n",
        "    failed = 0\n",
        "\n",
        "    for i, src_path in enumerate(files, 1):\n",
        "        rel_path = os.path.relpath(src_path, SRC_ROOT)\n",
        "        dst_path = os.path.join(DST_ROOT, rel_path)\n",
        "\n",
        "        try:\n",
        "            raw = read_text(src_path)\n",
        "            norm = normalize_with_tokens(raw)\n",
        "            write_text(dst_path, norm)\n",
        "            processed += 1\n",
        "            if i % 200 == 0:\n",
        "                print(f\"[{i}/{len(files)}] Processed so far: {processed}, Skipped: {skipped}, Failed: {failed}\")\n",
        "        except UnicodeDecodeError:\n",
        "            skipped += 1\n",
        "            print(f\"SKIP (decode): {src_path}\")\n",
        "        except Exception as e:\n",
        "            failed += 1\n",
        "            print(f\"ERROR ({type(e).__name__}): {src_path} -> {e}\")\n",
        "\n",
        "    print(f\"Done. Processed: {processed}, Skipped(decode): {skipped}, Failed: {failed}\")\n",
        "    print(f\"Output root: {DST_ROOT}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "NqpPHwWPtDRq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Read texts from text_selected_norm, analyze with spaCy, and save as .spacy files at OANC/spacy<br>\n",
        "This may take a couple of minutes."
      ],
      "metadata": {
        "id": "kYrPT7-JQrGC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the following for the local environment\n",
        "# pip install -U spacy\n",
        "# python -m spacy download en_core_web_sm\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import gc\n",
        "import spacy\n",
        "from spacy.tokens import DocBin\n",
        "import multiprocessing as mp\n",
        "\n",
        "# ===== Check Environment =====\n",
        "try:\n",
        "    import google.colab  # noqa: F401\n",
        "    IN_COLAB = True\n",
        "except ImportError:\n",
        "    IN_COLAB = False\n",
        "\n",
        "# ===== Common Settings =====\n",
        "MODEL = \"en_core_web_sm\"\n",
        "N_PROCESS = 4\n",
        "BATCH_SIZE = 20\n",
        "RELOAD_MB_THRESHOLD = 100            # Threshold in MB (e.g., reload every 100MB)\n",
        "RELOAD_EVERY = 50\n",
        "OUTPUT_SUBDIR = \"spacy\"\n",
        "\n",
        "# ===== Common Utilities =====\n",
        "def build_nlp():\n",
        "    nlp = spacy.load(MODEL, exclude=[\"ner\", \"textcat\"])\n",
        "    if \"sentencizer\" not in nlp.pipe_names:\n",
        "        nlp.add_pipe(\"sentencizer\", first=True)\n",
        "    return nlp\n",
        "\n",
        "def iter_nonempty_lines(path):\n",
        "    with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
        "        for line in f:\n",
        "            s = line.strip()\n",
        "            if s:\n",
        "                yield s\n",
        "\n",
        "def save_parsed_docs(input_path, output_path, nlp, n_process=2, batch_size=20):\n",
        "    stats = {\"lines\": 0, \"docs\": 0, \"words\": 0, \"per_doc_words\": []}\n",
        "    doc_bin = DocBin(store_user_data=False)\n",
        "    # Create a doc for each line break\n",
        "    for doc in nlp.pipe(iter_nonempty_lines(input_path), n_process=n_process, batch_size=batch_size):\n",
        "        doc_bin.add(doc)\n",
        "        stats[\"docs\"] += 1\n",
        "        w = sum(1 for t in doc if t.is_alpha)\n",
        "        stats[\"words\"] += w\n",
        "        stats[\"per_doc_words\"].append(w)\n",
        "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
        "    print(output_path)\n",
        "    doc_bin.to_disk(output_path)\n",
        "    return stats\n",
        "\n",
        "def list_txt_recursive(root_dir):\n",
        "    out = []\n",
        "    for cur, _, files in os.walk(root_dir):\n",
        "        for fn in files:\n",
        "            if fn.lower().endswith(\".txt\"):\n",
        "                out.append(os.path.join(cur, fn))\n",
        "    out.sort()\n",
        "    return out\n",
        "\n",
        "_prev_len = 0\n",
        "def status_line(msg: str):\n",
        "    global _prev_len\n",
        "    s = f\"\\r{msg}\"\n",
        "    pad = \" \" * max(0, _prev_len - len(msg))\n",
        "    sys.stdout.write(s + pad)\n",
        "    sys.stdout.flush()\n",
        "    _prev_len = len(msg)\n",
        "\n",
        "def to_spacy_out_path(txt_abs_path, input_root, output_root, in_colab, base_dir=None):\n",
        "    rel = os.path.relpath(txt_abs_path, input_root)\n",
        "    rel_no_ext = os.path.splitext(rel)[0]\n",
        "    if in_colab:\n",
        "        return os.path.normpath(os.path.join(output_root, rel_no_ext + \".spacy\"))\n",
        "    else:\n",
        "        return os.path.join(output_root, OUTPUT_SUBDIR, rel_no_ext + \".spacy\")\n",
        "\n",
        "def run_pipeline(input_dirs, output_dir, in_colab, base_dir, n_process, log_func=print, cancel_checker=None):\n",
        "    processed_file_n = 0\n",
        "    txt_files = []\n",
        "    file_map = {}\n",
        "    for root in input_dirs:\n",
        "        if os.path.isdir(root):\n",
        "            files = list_txt_recursive(root)\n",
        "            txt_files.extend(files)\n",
        "            for f in files:\n",
        "                file_map[f] = root\n",
        "        else:\n",
        "            log_func(f\"WARNING: Not found: {root}\")\n",
        "\n",
        "    if not txt_files:\n",
        "        log_func(f\"ERROR: No .txt files found under: {input_dirs}\")\n",
        "        return\n",
        "\n",
        "    info = f\"{len(input_dirs)} target folders\" if in_colab else f\"'{input_dirs[0]}'\"\n",
        "    log_func(f\"Processing {len(txt_files)} files from {info} -> {output_dir}\")\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    nlp = build_nlp()\n",
        "    start_all = time.time()\n",
        "    base_for_rel = (base_dir if in_colab else input_dirs[0])\n",
        "    bytes_since_reload = 0\n",
        "    threshold_bytes = RELOAD_MB_THRESHOLD * 1024 * 1024\n",
        "\n",
        "    for i, in_path in enumerate(txt_files, 1):\n",
        "        if cancel_checker and cancel_checker():\n",
        "            log_func(\"\\nProcessing cancelled by user.\")\n",
        "            break\n",
        "        iter_start = time.time()\n",
        "        out_path = to_spacy_out_path(in_path, file_map[in_path], output_dir, in_colab, base_dir)\n",
        "\n",
        "        # Update size; reload when over threshold\n",
        "        try:\n",
        "            sz = os.path.getsize(in_path)\n",
        "        except OSError:\n",
        "            sz = 0\n",
        "        bytes_since_reload += sz\n",
        "\n",
        "        stats = save_parsed_docs(\n",
        "            input_path=in_path,\n",
        "            output_path=out_path,\n",
        "            nlp=nlp,\n",
        "            n_process=n_process,\n",
        "            batch_size=BATCH_SIZE\n",
        "        )\n",
        "        # Progress display\n",
        "        elapsed_iter = time.time() - iter_start\n",
        "        elapsed_total = time.time() - start_all\n",
        "        rel_show = os.path.relpath(in_path, base_for_rel)\n",
        "\n",
        "        avg_w = (stats[\"words\"] / stats[\"docs\"]) if stats[\"docs\"] else 0\n",
        "        log_func(\n",
        "            f\"[{i}/{len(txt_files)}] {rel_show} \"\n",
        "            f\"last: {elapsed_iter:.2f}s | total: {elapsed_total:.2f}s | \"\n",
        "            f\"lines: {stats['docs']:,} | words: {stats['words']:,} | words/line: {avg_w:.1f}\"\n",
        "        )\n",
        "\n",
        "        # === Reload Conditions ====\n",
        "        need_reload = False\n",
        "        if RELOAD_EVERY and (i % RELOAD_EVERY == 0):\n",
        "            need_reload = True\n",
        "        if bytes_since_reload >= threshold_bytes:\n",
        "            need_reload = True\n",
        "\n",
        "        if need_reload:\n",
        "            nlp = build_nlp()\n",
        "            bytes_since_reload = 0\n",
        "            gc.collect()\n",
        "            log_func(\"\\nReloaded!\")\n",
        "        processed_file_n = i\n",
        "\n",
        "    total_time = time.time() - start_all\n",
        "    log_func(f\"\\n{processed_file_n} files processed in {total_time:.2f} seconds.\")\n",
        "\n",
        "# ===== Entry Point =====\n",
        "def main():\n",
        "    if IN_COLAB:\n",
        "        base_dir = \"/content/dep_collocation_tutorial/OANC\"\n",
        "        #base_dir = \"/content/OANC\"\n",
        "        input_dirs = [\n",
        "            os.path.join(base_dir, \"text_selected_norm\"),\n",
        "        ]\n",
        "        output_dir = os.path.join(base_dir, \"spacy\")\n",
        "        n_proc = 1\n",
        "        run_pipeline(input_dirs, output_dir, in_colab=True, base_dir=base_dir, n_process=n_proc)\n",
        "    else:\n",
        "        # --- Local GUI: Set input, output, and processes ---\n",
        "        import tkinter as tk\n",
        "        from tkinter import filedialog, messagebox, ttk\n",
        "        import threading\n",
        "\n",
        "        class App(tk.Tk):\n",
        "            def __init__(self):\n",
        "                super().__init__()\n",
        "                self.title(\"spaCyDataCreator\")\n",
        "                self.geometry(\"780x580\")\n",
        "\n",
        "                self.input_dir = tk.StringVar()\n",
        "                self.output_dir = tk.StringVar()\n",
        "\n",
        "                cores = os.cpu_count() or 1\n",
        "                upper = max(1, cores - 2)      # Maximum = CPU cores minus 2 (at least 1)\n",
        "                suggested = min(max(1, cores - 1), upper)  # Recommended = cores - 1 (capped at max)\n",
        "                self.upper = upper\n",
        "                self.n_proc = tk.IntVar(value=1)\n",
        "\n",
        "                pad = {\"padx\": 10, \"pady\": 8}\n",
        "\n",
        "                # Input\n",
        "                frm_in = ttk.LabelFrame(self, text=\"Input\")\n",
        "                frm_in.pack(fill=\"x\", **pad)\n",
        "                ttk.Button(frm_in, text=\"Input folder\", command=self.select_input).pack(anchor=\"w\", padx=10, pady=(10, 4))\n",
        "                ttk.Entry(frm_in, textvariable=self.input_dir, state=\"readonly\").pack(fill=\"x\", padx=10, pady=(0, 10))\n",
        "\n",
        "                # Output\n",
        "                frm_out = ttk.LabelFrame(self, text=\"Output\")\n",
        "                frm_out.pack(fill=\"x\", **pad)\n",
        "                ttk.Button(frm_out, text=\"Output folder\", command=self.select_output).pack(anchor=\"w\", padx=10, pady=(10, 4))\n",
        "                ttk.Entry(frm_out, textvariable=self.output_dir, state=\"readonly\").pack(fill=\"x\", padx=10, pady=(0, 10))\n",
        "\n",
        "                # Process\n",
        "                frm_np = ttk.LabelFrame(self, text=\"Processes\")\n",
        "                frm_np.pack(fill=\"x\", **pad)\n",
        "                row = ttk.Frame(frm_np); row.pack(fill=\"x\", padx=10, pady=(8, 10))\n",
        "                ttk.Label(row, text=f\"n_process (1–{upper}):\").pack(side=\"left\")\n",
        "                self.spin = ttk.Spinbox(row, from_=1, to=upper, textvariable=self.n_proc, width=6, justify=\"center\")\n",
        "                self.spin.pack(side=\"left\", padx=8)\n",
        "                ttk.Label(frm_np, text=\"Try smaller values first.\").pack(anchor=\"w\", padx=12, pady=(0, 8))\n",
        "\n",
        "                # Start\n",
        "                frm_start = ttk.Frame(self);\n",
        "                frm_start.pack(fill=\"x\", **pad)\n",
        "                self.btn_start = ttk.Button(frm_start, text=\"Start\", command=self.on_start, state=\"disabled\")\n",
        "                self.btn_start.pack(side=\"left\")\n",
        "\n",
        "                # Cancel\n",
        "                self.cancel_flag = threading.Event()\n",
        "                self.btn_cancel = ttk.Button(frm_start, text=\"Cancel\", command=self.on_cancel, state=\"disabled\")\n",
        "                self.btn_cancel.pack(side=\"left\", padx=(8, 0))\n",
        "\n",
        "                self.lbl_status = ttk.Label(frm_start, text=\"Select input/output and processes.\")\n",
        "                self.lbl_status.pack(side=\"left\", padx=10)\n",
        "\n",
        "                # Log + Scrollbar\n",
        "                frm_log = ttk.LabelFrame(self, text=\"Log\")\n",
        "                frm_log.pack(fill=\"both\", expand=True, **pad)\n",
        "                cnt = ttk.Frame(frm_log); cnt.pack(fill=\"both\", expand=True, padx=10, pady=10)\n",
        "                cnt.grid_rowconfigure(0, weight=1)\n",
        "                cnt.grid_columnconfigure(0, weight=1)\n",
        "                self.txt_log = tk.Text(cnt, height=18, wrap=\"none\")\n",
        "                self.txt_log.grid(row=0, column=0, sticky=\"nsew\")\n",
        "                scroll_y = ttk.Scrollbar(cnt, orient=\"vertical\", command=self.txt_log.yview)\n",
        "                scroll_y.grid(row=0, column=1, sticky=\"ns\")\n",
        "                self.txt_log.configure(yscrollcommand=scroll_y.set)\n",
        "\n",
        "            def select_input(self):\n",
        "                p = filedialog.askdirectory(title=\"Select input folder (recursive .txt)\")\n",
        "                if p:\n",
        "                    if not list_txt_recursive(p):\n",
        "                        messagebox.showwarning(\"Warning\", f\"No .txt files found under:\\n{p}\\nSelect another folder.\")\n",
        "                        return\n",
        "                    self.input_dir.set(p)\n",
        "                    self.update_start_state()\n",
        "\n",
        "            def select_output(self):\n",
        "                p = filedialog.askdirectory(title=\"Select output folder (same as input is OK)\")\n",
        "                if p:\n",
        "                    self.output_dir.set(p)\n",
        "                    self.update_start_state()\n",
        "\n",
        "            def update_start_state(self):\n",
        "                in_p, out_p = self.input_dir.get(), self.output_dir.get()\n",
        "                ok = bool(in_p and out_p)\n",
        "                self.btn_start.config(state=(tk.NORMAL if ok else tk.DISABLED))\n",
        "\n",
        "            def log(self, msg: str):\n",
        "                self.txt_log.insert(\"end\", msg + \"\\n\")\n",
        "                self.txt_log.see(\"end\")\n",
        "                self.update_idletasks()\n",
        "\n",
        "            def on_start(self):\n",
        "                in_p, out_p = self.input_dir.get().strip(), self.output_dir.get().strip()\n",
        "                if not in_p or not out_p:\n",
        "                    messagebox.showwarning(\"Warning\", \"Both input and output folders must be selected.\")\n",
        "                    return\n",
        "                txts = list_txt_recursive(in_p)\n",
        "                if not txts:\n",
        "                    messagebox.showwarning(\"Warning\", f\"No .txt files under:\\n{in_p}\")\n",
        "                    return\n",
        "\n",
        "                try:\n",
        "                    n_proc = int(self.n_proc.get())\n",
        "                except Exception:\n",
        "                    messagebox.showwarning(\"Warning\", \"n_process must be an integer.\")\n",
        "                    return\n",
        "                if not (1 <= n_proc <= self.upper):\n",
        "                    messagebox.showwarning(\"Warning\", f\"n_process must be between 1 and {self.upper}.\")\n",
        "                    return\n",
        "\n",
        "                self.btn_start.config(state=\"disabled\")\n",
        "                self.lbl_status.config(text=f\"Running... (n_process={n_proc})\")\n",
        "                self.cancel_flag.clear()\n",
        "                self.btn_cancel.config(state=\"normal\")\n",
        "\n",
        "                def _runner():\n",
        "                    try:\n",
        "                        run_pipeline(\n",
        "                            [in_p], out_p, in_colab=False, base_dir=None, n_process=n_proc,\n",
        "                            log_func=self.log, cancel_checker=self.check_cancel\n",
        "                        )\n",
        "                        if self.check_cancel():\n",
        "                            self.lbl_status.config(text=\"Cancelled.\")\n",
        "                        else:\n",
        "                            self.lbl_status.config(text=\"Done.\")\n",
        "                    except Exception as e:\n",
        "                        self.log(f\"[ERROR] {e}\")\n",
        "                        messagebox.showerror(\"Error\", str(e))\n",
        "                        self.lbl_status.config(text=\"Error.\")\n",
        "                    finally:\n",
        "                        self.btn_start.config(state=\"normal\")\n",
        "                        self.btn_cancel.config(state=\"disabled\")\n",
        "                        self.cancel_flag.clear()\n",
        "\n",
        "                import threading\n",
        "                threading.Thread(target=_runner, daemon=True).start()\n",
        "\n",
        "            def on_cancel(self):\n",
        "                self.cancel_flag.set()\n",
        "                self.lbl_status.config(text=\"Cancelling...may take a whole\")\n",
        "\n",
        "            def check_cancel(self):\n",
        "                return self.cancel_flag.is_set()\n",
        "\n",
        "        App().mainloop()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    mp.freeze_support() # Safety measure for multiprocessing with Windows/PyInstaller\n",
        "\n",
        "    main()"
      ],
      "metadata": {
        "id": "f7AV6CjvQtvS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#6. Calculate MI and t-score\n",
        "Read pair_all.csv and caliculate frequencies and association strength scores based on OANC."
      ],
      "metadata": {
        "id": "blWHlV4dSsPt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, re, math, time\n",
        "import pandas as pd\n",
        "import spacy\n",
        "from spacy.tokens import DocBin\n",
        "try:\n",
        "    # colab environment\n",
        "    # Comment out if not using with Google Drive\n",
        "    #from google.colab import drive\n",
        "    #drive.mount(\"/content/drive\", force_remount=False)\n",
        "    _IN_COLAB = True\n",
        "except Exception:\n",
        "    drive = None\n",
        "    _IN_COLAB = False\n",
        "\n",
        "import shutil, os\n",
        "\n",
        "import sys\n",
        "if _IN_COLAB:\n",
        "    #sys.path.append('/content/drive/MyDrive/dep_collocation_tutorial')\n",
        "    sys.path.append('/content/dep_collocation_tutorial')\n",
        "try:\n",
        "    import mylib\n",
        "except Exception as e:\n",
        "    if not _IN_COLAB:\n",
        "        sys.path.append(os.getcwd())\n",
        "        try:\n",
        "            import mylib\n",
        "        except Exception as e2:\n",
        "            raise ImportError(\n",
        "                \"Failed to import mylib. On local environments, place mylib.py in the same folder as the script\"\n",
        "            ) from e2\n",
        "    else:\n",
        "        raise\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# Configuration (Specify the paths)\n",
        "# ------------------------------------------------------------\n",
        "# Input CSV path\n",
        "#CSV_PATH = \"/content/drive/MyDrive/dep_collocation_tutorial/collocation/pairs_all.csv\"\n",
        "CSV_PATH = \"/content/dep_collocation_tutorial/collocation/pairs_all.csv\"\n",
        "\n",
        "# Reference corpus folder path (with .spacy files)\n",
        "#REF_FOLDER = \"/content/drive/MyDrive/dep_collocation_tutorial/OANC/spacy\"\n",
        "REF_FOLDER = \"/content/dep_collocation_tutorial/OANC/spacy\"\n",
        "\n",
        "# Output CSV file name (if not specified, defaults to input CSV name + '_scores.csv')\"\n",
        "OUTPUT_CSV_NAME = \"\"  # 例: \"pairs_all_scores.csv\"\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "# Copy temporarily to /content and process from there (recommended for Google Drive)\n",
        "USE_CACHE  = False\n",
        "CACHE_ROOT = \"/content/ref_spacy_cache\"\n",
        "\n",
        "if USE_CACHE:\n",
        "    if not os.path.isdir(REF_FOLDER):\n",
        "        raise FileNotFoundError(f\"Not found: {REF_FOLDER}\")\n",
        "\n",
        "    if \"/content/drive\" in REF_FOLDER:\n",
        "        # ===== Google Drive path: copy to cache =====\n",
        "        shutil.copytree(REF_FOLDER, CACHE_ROOT, dirs_exist_ok=True)\n",
        "        REF_FOLDER = CACHE_ROOT\n",
        "        print(\"Using cached input at:\", REF_FOLDER)\n",
        "    else:\n",
        "        # ===== Virtual drive path: use directly =====\n",
        "        print(\"Using virtual drive path directly:\", REF_FOLDER)\n",
        "\n",
        "def norm_pt(s: str) -> str:\n",
        "    return str(s).strip().upper()\n",
        "\n",
        "def norm_lemma(s: str) -> str:\n",
        "    s = str(s).strip().lower()\n",
        "    s = re.sub(r'^[^\\w]+|[^\\w]+$', '', s)\n",
        "    return s\n",
        "\n",
        "def iter_spacy_files(root):\n",
        "    # Recursively find .spacy files (include subfolders)\n",
        "    paths = []\n",
        "    for dp, _, files in os.walk(root):\n",
        "        for f in files:\n",
        "            if f.lower().endswith(\".spacy\"):\n",
        "                paths.append(os.path.join(dp, f))\n",
        "    return sorted(paths)\n",
        "\n",
        "def collect_counts_from_ref_spacy(ref_folder, nlp, *, needed_rows, needed_left, needed_right, log_func):\n",
        "    pair_ref_freq, lemma1_ref_freq, lemma2_ref_freq = {}, {}, {}\n",
        "    total_ref_freq_by_type = {\"ADJ_NOUN\": 0, \"VERB_OBJ\": 0, \"VERB_ADV\": 0, \"ADV_VERB\": 0, \"ADV_ADJ\": 0}\n",
        "\n",
        "    spacy_files = iter_spacy_files(ref_folder)\n",
        "    if not spacy_files:\n",
        "        raise FileNotFoundError(\"No '.spacy' files were found in the reference corpus folder.。\")\n",
        "\n",
        "    total_files = len(spacy_files)\n",
        "    log_func(f\"Start loading reference corpus: {total_files} .spacy files\")\n",
        "\n",
        "    for file_idx, path in enumerate(spacy_files, start=1):\n",
        "        base = os.path.basename(path)\n",
        "        try:\n",
        "            db = DocBin().from_disk(path)\n",
        "        except Exception as e:\n",
        "            log_func(f\"⚠ Skipped (Filed loading): {base} :: {e}\")\n",
        "            continue\n",
        "\n",
        "        doc_counter = 0\n",
        "        for doc in db.get_docs(nlp.vocab):\n",
        "            doc_counter += 1\n",
        "\n",
        "            for token in doc:\n",
        "                '''\n",
        "                それぞれの依存ペアのif条件（ペア抽出条件）は、PairExtractorと同じにする\n",
        "                sent = token.sent を加える\n",
        "                '''\n",
        "                sent = token.sent # Needed to keep it consistent with PairExtracor.py code\n",
        "\n",
        "                # --- ADJ_NOUN ---\n",
        "                if mylib.is_adj_noun(token):\n",
        "                    l1, l2 = norm_lemma(token.lemma_), norm_lemma(token.head.lemma_)\n",
        "                    total_ref_freq_by_type[\"ADJ_NOUN\"] += 1\n",
        "                    if (\"ADJ_NOUN\", l1, l2) in needed_rows:\n",
        "                        pair_ref_freq[(\"ADJ_NOUN\", l1, l2)] = pair_ref_freq.get((\"ADJ_NOUN\", l1, l2), 0) + 1\n",
        "                    if l1 in needed_left.get(\"ADJ_NOUN\", set()):\n",
        "                        lemma1_ref_freq[(\"ADJ_NOUN\", l1)] = lemma1_ref_freq.get((\"ADJ_NOUN\", l1), 0) + 1\n",
        "                    if l2 in needed_right.get(\"ADJ_NOUN\", set()):\n",
        "                        lemma2_ref_freq[(\"ADJ_NOUN\", l2)] = lemma2_ref_freq.get((\"ADJ_NOUN\", l2), 0) + 1\n",
        "\n",
        "                # --- VERB_OBJ ---\n",
        "                if mylib.is_verb_obj(token):\n",
        "                    l1, l2 = norm_lemma(token.head.lemma_), norm_lemma(token.lemma_)\n",
        "                    total_ref_freq_by_type[\"VERB_OBJ\"] += 1\n",
        "                    if (\"VERB_OBJ\", l1, l2) in needed_rows:\n",
        "                        pair_ref_freq[(\"VERB_OBJ\", l1, l2)] = pair_ref_freq.get((\"VERB_OBJ\", l1, l2), 0) + 1\n",
        "                    if l1 in needed_left.get(\"VERB_OBJ\", set()):\n",
        "                        lemma1_ref_freq[(\"VERB_OBJ\", l1)] = lemma1_ref_freq.get((\"VERB_OBJ\", l1), 0) + 1\n",
        "                    if l2 in needed_right.get(\"VERB_OBJ\", set()):\n",
        "                        lemma2_ref_freq[(\"VERB_OBJ\", l2)] = lemma2_ref_freq.get((\"VERB_OBJ\", l2), 0) + 1\n",
        "\n",
        "                # --- VERB_ADV ---\n",
        "                if mylib.is_verb_adv(token):\n",
        "                    l1, l2 = norm_lemma(token.head.lemma_), norm_lemma(token.lemma_)\n",
        "                    total_ref_freq_by_type[\"VERB_ADV\"] += 1\n",
        "                    if (\"VERB_ADV\", l1, l2) in needed_rows:\n",
        "                        pair_ref_freq[(\"VERB_ADV\", l1, l2)] = pair_ref_freq.get((\"VERB_ADV\", l1, l2), 0) + 1\n",
        "                    if l1 in needed_left.get(\"VERB_ADV\", set()):\n",
        "                        lemma1_ref_freq[(\"VERB_ADV\", l1)] = lemma1_ref_freq.get((\"VERB_ADV\", l1), 0) + 1\n",
        "                    if l2 in needed_right.get(\"VERB_ADV\", set()):\n",
        "                        lemma2_ref_freq[(\"VERB_ADV\", l2)] = lemma2_ref_freq.get((\"VERB_ADV\", l2), 0) + 1\n",
        "\n",
        "                # --- ADV_VERB ---\n",
        "                if mylib.is_adv_verb(token, sent):\n",
        "                    l1, l2 = norm_lemma(token.lemma_), norm_lemma(token.head.lemma_)\n",
        "                    total_ref_freq_by_type[\"ADV_VERB\"] += 1\n",
        "                    if (\"ADV_VERB\", l1, l2) in needed_rows:\n",
        "                        pair_ref_freq[(\"ADV_VERB\", l1, l2)] = pair_ref_freq.get((\"ADV_VERB\", l1, l2), 0) + 1\n",
        "                    if l1 in needed_left.get(\"ADV_VERB\", set()):\n",
        "                        lemma1_ref_freq[(\"ADV_VERB\", l1)] = lemma1_ref_freq.get((\"ADV_VERB\", l1), 0) + 1\n",
        "                    if l2 in needed_right.get(\"ADV_VERB\", set()):\n",
        "                        lemma2_ref_freq[(\"ADV_VERB\", l2)] = lemma2_ref_freq.get((\"ADV_VERB\", l2), 0) + 1\n",
        "\n",
        "                # --- ADV_ADJ ---\n",
        "                if mylib.is_adv_adj(token):\n",
        "                    l1, l2 = norm_lemma(token.lemma_), norm_lemma(token.head.lemma_)\n",
        "                    total_ref_freq_by_type[\"ADV_ADJ\"] += 1\n",
        "                    if (\"ADV_ADJ\", l1, l2) in needed_rows:\n",
        "                        pair_ref_freq[(\"ADV_ADJ\", l1, l2)] = pair_ref_freq.get((\"ADV_ADJ\", l1, l2), 0) + 1\n",
        "                    if l1 in needed_left.get(\"ADV_ADJ\", set()):\n",
        "                        lemma1_ref_freq[(\"ADV_ADJ\", l1)] = lemma1_ref_freq.get((\"ADV_ADJ\", l1), 0) + 1\n",
        "                    if l2 in needed_right.get(\"ADV_ADJ\", set()):\n",
        "                        lemma2_ref_freq[(\"ADV_ADJ\", l2)] = lemma2_ref_freq.get((\"ADV_ADJ\", l2), 0) + 1\n",
        "\n",
        "        msg = f\"\\r[{file_idx}/{total_files}] {base}: {doc_counter} docs\"\n",
        "        if log_func is print:\n",
        "            print(\"\\r\" + msg, end=\"\", flush=True)\n",
        "        else:\n",
        "            log_func(msg)\n",
        "\n",
        "    return pair_ref_freq, lemma1_ref_freq, lemma2_ref_freq, total_ref_freq_by_type\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def add_assoc_scores(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    # Convert required columns to float\n",
        "    f_xy = df[\"pair_ref_freq\"].astype(float).to_numpy()\n",
        "    f_x  = df[\"lemma1_ref_freq\"].astype(float).to_numpy()\n",
        "    f_y  = df[\"lemma2_ref_freq\"].astype(float).to_numpy()\n",
        "    N_t  = df[\"total_ref_type_freq\"].astype(float).to_numpy()\n",
        "\n",
        "    # ===== MI =====\n",
        "    # MI = log2( (f_xy * N_t) / (f_x * f_y) )\n",
        "    mi = np.full(len(df), np.nan, dtype=float)\n",
        "    valid_mi = (f_xy > 0) & (f_x > 0) & (f_y > 0) & (N_t > 0)\n",
        "    mi[valid_mi] = np.log2((f_xy[valid_mi] * N_t[valid_mi]) / (f_x[valid_mi] * f_y[valid_mi]))\n",
        "\n",
        "    # ===== t-score =====\n",
        "    # t = (f_xy - (f_x*f_y)/N_t) / sqrt(f_xy)\n",
        "    tscore = np.full(len(df), np.nan, dtype=float)\n",
        "    valid_t = (f_xy > 0) & (f_x > 0) & (f_y > 0) & (N_t > 0)\n",
        "    expected = np.zeros(len(df), dtype=float)\n",
        "    expected[valid_t] = (f_x[valid_t] * f_y[valid_t]) / N_t[valid_t]\n",
        "    tscore[valid_t] = (f_xy[valid_t] - expected[valid_t]) / np.sqrt(f_xy[valid_t])\n",
        "\n",
        "    # Update the DataFrame\n",
        "    df[\"MI\"] = mi\n",
        "    df[\"t_score\"] = tscore\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "def run_pipeline(csv_path, ref_folder, log_func=print):\n",
        "    global CSV_PATH, REF_FOLDER\n",
        "    CSV_PATH = csv_path\n",
        "    REF_FOLDER = ref_folder\n",
        "\n",
        "    if not os.path.isfile(CSV_PATH):\n",
        "        raise FileNotFoundError(f\"Input CSV not found.: {CSV_PATH}\")\n",
        "    if not os.path.isdir(REF_FOLDER):\n",
        "        raise FileNotFoundError(f\"Reference corpus folder not found.: {REF_FOLDER}\")\n",
        "\n",
        "    # Read pair_all.csv (extracted from learner data) into a DataFrame\n",
        "    df = pd.read_csv(CSV_PATH, encoding=\"utf-8-sig\", dtype=str, na_filter=False)\n",
        "\n",
        "    # Check required columns\n",
        "    for col in [\"pair_type\",\"lemma1\",\"lemma2\"]:\n",
        "        if col not in df.columns:\n",
        "            raise ValueError(f\"Missing required column(s).: {col}\")\n",
        "\n",
        "    # Normalization\n",
        "    df[\"pair_type\"] = df[\"pair_type\"].map(norm_pt)\n",
        "    df[\"lemma1\"]    = df[\"lemma1\"].map(norm_lemma)\n",
        "    df[\"lemma2\"]    = df[\"lemma2\"].map(norm_lemma)\n",
        "\n",
        "    #Create a deduplicated list\n",
        "    needed_rows  = {(pt, l1, l2) for pt, l1, l2 in zip(df[\"pair_type\"], df[\"lemma1\"], df[\"lemma2\"]) }\n",
        "    needed_left, needed_right = {}, {}\n",
        "    for pt, l1, l2 in needed_rows:\n",
        "        needed_left.setdefault(pt, set()).add(l1)\n",
        "        needed_right.setdefault(pt, set()).add(l2)\n",
        "\n",
        "    # Since DocBin is already parsed, load only the minimal model\n",
        "    nlp = spacy.blank(\"en\")\n",
        "\n",
        "    start_time = time.perf_counter()\n",
        "\n",
        "    # Get frequencies from the reference corpus\n",
        "    # pair_counts   = frequency of the pair (e.g., frequency of \"big problem\")\n",
        "    # marg1_counts  = frequency of the left element (e.g., frequency of \"big\")\n",
        "    # marg2_counts  = frequency of the right element (e.g., frequency of \"problem\")\n",
        "    # total_by_type = frequency of the pair type (e.g., frequency of ADJ_NOUN)\n",
        "    pair_counts, marg1_counts, marg2_counts, total_by_type = collect_counts_from_ref_spacy(\n",
        "        REF_FOLDER, nlp,\n",
        "        needed_rows=needed_rows,\n",
        "        needed_left=needed_left,\n",
        "        needed_right=needed_right\n",
        "        ,log_func = log_func\n",
        "    )\n",
        "\n",
        "    '''\n",
        "    After running collect_counts_from_ref_spacy, variables such as pair_counts will contain values like the following:\n",
        "    pair_counts = {\n",
        "        (\"ADJ_NOUN\", \"big\", \"problem\"): 25,\n",
        "        (\"VERB_OBJ\", \"eat\", \"apple\"): 15,\n",
        "    }\n",
        "    marg1_counts = {\n",
        "        (\"ADJ_NOUN\", \"big\"): 50,\n",
        "    }\n",
        "    '''\n",
        "    # Initialize list\n",
        "    lemma1_freqs, lemma2_freqs, pair_freqs, total_freqs = [], [], [], []\n",
        "\n",
        "    # Example of DataFrame df (after loading the CSV)\n",
        "    # ------------------------------------------------------\n",
        "    # file     pair_type   lemma1   lemma2   sentence ...\n",
        "    # 001.txt  ADJ_NOUN    big      problem  \"It is a big problem.\"\n",
        "    # 002.txt  VERB_OBJ    eat      apple    \"I eat an apple.\"\n",
        "    # 003.txt  ADV_VERB    quickly  run      \"He quickly runs.\"\n",
        "    # ------------------------------------------------------\n",
        "\n",
        "    # Use itertuples for row-wise processing\n",
        "    for i, row in enumerate(df.itertuples(index=False), start=1):\n",
        "        # From each row, get the dependency type and the left/right lemmas, then normalize\n",
        "        pt  = norm_pt(row.pair_type)   # e.g., \"ADJ_NOUN\"\n",
        "        l1p = norm_lemma(row.lemma1)   # e.g., \"big\"\n",
        "        l2p = norm_lemma(row.lemma2)   # e.g., \"problem\"\n",
        "\n",
        "        # --- Retrieve co-occurrence and marginal frequencies from reference tables ---\n",
        "\n",
        "        # f_xy: frequency of the pair (dependency type, lemma1, lemma2)\n",
        "        #   e.g., (\"ADJ_NOUN\", \"big\", \"problem\") → 25 times\n",
        "        # Retrieve the value using the tuple (pt, l1p, l2p) as the key\n",
        "        f_xy = pair_counts.get((pt, l1p, l2p), 0)\n",
        "\n",
        "        # f_x: left-side frequency (marginal) of (dependency type, lemma1)\n",
        "        #   e.g., (\"ADJ_NOUN\", \"big\") → 50 times (total count of \"big\" modifying nouns)\n",
        "        f_x  = marg1_counts.get((pt, l1p), 0)\n",
        "\n",
        "        # f_y: right-side frequency (marginal) of (dependency type, lemma2)\n",
        "        #   e.g., (\"ADJ_NOUN\", \"problem\") → 120 times (total count of \"problem\" being modified by adjectives)\n",
        "        f_y  = marg2_counts.get((pt, l2p), 0)\n",
        "\n",
        "        # N_t: total token count for the dependency type\n",
        "        #   e.g., (\"ADJ_NOUN\") → 10,000 times across the entire corpus\n",
        "        N_t  = total_by_type.get(pt, 0)\n",
        "\n",
        "        # --- Append to lists for later use as DataFrame columns ---\n",
        "        lemma1_freqs.append(f_x)   # marginal frequency of the left element\n",
        "        lemma2_freqs.append(f_y)   # marginal frequency of the right element\n",
        "        pair_freqs.append(f_xy)    # co-occurrence frequency of the pair itself\n",
        "        total_freqs.append(N_t)    # total count of the dependency type\n",
        "\n",
        "    # After the loop, add new columns to the original DataFrame df\n",
        "    df[\"lemma1_ref_freq\"]      = lemma1_freqs       # marginal frequency of the left lemma\n",
        "    df[\"lemma2_ref_freq\"]      = lemma2_freqs       # marginal frequency of the right lemma\n",
        "    df[\"pair_ref_freq\"]        = pair_freqs         # co-occurrence frequency of the pair\n",
        "    df[\"total_ref_type_freq\"]  = total_freqs        # total frequency of the dependency type\n",
        "    df = add_assoc_scores(df)  # add association strength scores\n",
        "\n",
        "    log_func(f\"✓ Finished: {len(df)} rows, time={time.perf_counter()-start_time:.2f}s\")\n",
        "\n",
        "    if OUTPUT_CSV_NAME:\n",
        "        out_path = os.path.join(os.path.dirname(CSV_PATH), OUTPUT_CSV_NAME)\n",
        "    else:\n",
        "        out_path = f\"{os.path.splitext(CSV_PATH)[0]}_scores.csv\"\n",
        "\n",
        "    df.to_csv(out_path, index=False, encoding=\"utf-8-sig\")\n",
        "    print(f\"✓ Created output file: {out_path}\")\n",
        "    return out_path\n",
        "\n",
        "def main():\n",
        "    if _IN_COLAB:\n",
        "        #if not os.path.exists(\"/content/drive\"):\n",
        "            #drive.mount(\"/content/drive\")\n",
        "        #  In Colab, run with the default path as is\n",
        "        run_pipeline(CSV_PATH, REF_FOLDER)\n",
        "    else:\n",
        "        # --- Local uses GUI ---\n",
        "        import threading\n",
        "        import tkinter as tk\n",
        "        from tkinter import ttk, filedialog, messagebox\n",
        "\n",
        "        class App(tk.Tk):\n",
        "            def __init__(self):\n",
        "                super().__init__()\n",
        "                self.title(\"ScoreCalculator\")\n",
        "                self.geometry(\"720x520\")\n",
        "                # 値は StringVar で保持\n",
        "                self.csv_path = tk.StringVar()\n",
        "                self.ref_dir  = tk.StringVar()\n",
        "\n",
        "                pad = {\"padx\": 10, \"pady\": 8}\n",
        "\n",
        "                frm_in = ttk.LabelFrame(self, text=\"Input\")\n",
        "                frm_in.pack(fill=\"x\", **pad)\n",
        "\n",
        "                # Input file (.csv)\n",
        "                ttk.Button(frm_in, text=\"Input file (.csv)\", command=self.select_csv)\\\n",
        "                    .pack(anchor=\"w\", padx=10, pady=(10,4))\n",
        "                ttk.Entry(frm_in, textvariable=self.csv_path, state=\"readonly\")\\\n",
        "                    .pack(fill=\"x\", padx=10, pady=(0,10))\n",
        "\n",
        "                # Reference folder (spacy)\n",
        "                ttk.Button(frm_in, text=\"Reference folder (spacy)\", command=self.select_ref)\\\n",
        "                    .pack(anchor=\"w\", padx=10, pady=(10,4))\n",
        "                ttk.Label(frm_in, text=\"Select \\\"spacy\\\" folder\").pack(anchor=\"w\", padx=10, pady=(0, 10))\n",
        "                ttk.Entry(frm_in, textvariable=self.ref_dir, state=\"readonly\")\\\n",
        "                    .pack(fill=\"x\", padx=10, pady=(0,10))\n",
        "\n",
        "                frm_start = ttk.Frame(self); frm_start.pack(fill=\"x\", **pad)\n",
        "                self.btn_start = ttk.Button(frm_start, text=\"Start\", command=self.on_start, state=\"disabled\")\n",
        "                self.btn_start.pack(side=\"left\")\n",
        "                self.lbl_status = ttk.Label(frm_start, text=\"Select CSV and reference folder.\")\n",
        "                self.lbl_status.pack(side=\"left\", padx=10)\n",
        "\n",
        "                frm_log = ttk.LabelFrame(self, text=\"Log\")\n",
        "                frm_log.pack(fill=\"both\", expand=True, padx=10, pady=10)\n",
        "                cnt = ttk.Frame(frm_log)\n",
        "                cnt.pack(fill=\"both\", expand=True)\n",
        "                self.txt_log = tk.Text(cnt, height=14, wrap=\"none\")\n",
        "                self.txt_log.pack(side=\"left\", fill=\"both\", expand=True)\n",
        "                self.scroll_y = ttk.Scrollbar(cnt, orient=\"vertical\", command=self.txt_log.yview)\n",
        "                self.scroll_y.pack(side=\"right\", fill=\"y\")\n",
        "                self.txt_log.configure(yscrollcommand=self.scroll_y.set)\n",
        "\n",
        "            def select_csv(self):\n",
        "                path = filedialog.askopenfilename(\n",
        "                    title=\"Select CSV\",\n",
        "                    filetypes=[(\"CSV files\",\"*.csv\"), (\"All files\",\"*.*\")]\n",
        "                )\n",
        "                if path:\n",
        "                    self.csv_path.set(path)\n",
        "                    self._update_start_state()\n",
        "\n",
        "            def select_ref(self):\n",
        "                path = filedialog.askdirectory(\n",
        "                    title=\"Select the reference corpus folder (.spacy files)\"\n",
        "                )\n",
        "                if path:\n",
        "                    # Check if the folder name is \"spacy\"\n",
        "                    if os.path.basename(path).lower() != \"spacy\":\n",
        "                        messagebox.showwarning(\n",
        "                            \"Invalid folder\",\n",
        "                            \"The selected folder is not named 'spacy'.\\nPlease select the correct 'spacy' folder.\"\n",
        "                        )\n",
        "                        self.select_ref()\n",
        "                        return\n",
        "                    # Recursively search for .spacy files (including subfolders)\n",
        "                    spacy_files = []\n",
        "                    for root, _, files in os.walk(path):\n",
        "                        for f in files:\n",
        "                            if f.lower().endswith(\".spacy\"):\n",
        "                                spacy_files.append(os.path.join(root, f))\n",
        "\n",
        "                    if not spacy_files:\n",
        "                        messagebox.showwarning(\n",
        "                            \"No .spacy files\",\n",
        "                            \"The selected folder does not contain any .spacy files.\\nPlease select the correct folder.\"\n",
        "                        )\n",
        "                        # Ask the user to reselect\n",
        "                        self.select_ref()\n",
        "                        return\n",
        "\n",
        "                    self.ref_dir.set(path)\n",
        "                    self._update_start_state()\n",
        "\n",
        "            def _update_start_state(self):\n",
        "                if self.csv_path.get() and self.ref_dir.get():\n",
        "                    self.btn_start.configure(state=\"normal\")\n",
        "                    self.lbl_status.configure(text=\"Ready to start.\")\n",
        "                else:\n",
        "                    self.btn_start.configure(state=\"disabled\")\n",
        "                    self.lbl_status.configure(text=\"Select CSV and reference folder.\")\n",
        "\n",
        "            def on_start(self):\n",
        "                self.btn_start.configure(state=\"disabled\")\n",
        "                self.lbl_status.configure(text=\"Processing...\")\n",
        "                self.log(\"Processing started...\\n\")\n",
        "\n",
        "                def worker():\n",
        "                    try:\n",
        "                        out_path = run_pipeline(self.csv_path.get(), self.ref_dir.get(), log_func=self.log)\n",
        "                        self.log(f\"Done: {out_path}\\n\")\n",
        "                        messagebox.showinfo(\"Done\", f\"Finished:\\n{out_path}\")\n",
        "                    except Exception as e:\n",
        "                        self._append_log(f\"Error: {e}\\n\")\n",
        "                        messagebox.showerror(\"Error\", str(e))\n",
        "                    finally:\n",
        "                        self.btn_start.configure(state=\"normal\")\n",
        "                        self.lbl_status.configure(text=\"Finished.\")\n",
        "\n",
        "                threading.Thread(target=worker, daemon=True).start()\n",
        "\n",
        "            def log(self, msg):\n",
        "                self.txt_log.insert(\"end\", msg + \"\\n\");\n",
        "                self.txt_log.see(\"end\");\n",
        "                self.update_idletasks()\n",
        "\n",
        "\n",
        "        App().mainloop()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "VJx-gntiTIJL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}